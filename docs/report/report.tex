\documentclass[11pt, oneside]{article}

% Packages
\usepackage{sectsty}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage[super]{nth}
\usepackage{mathtools, amsfonts}

% Package config
\sectionfont{\normalfont\sffamily\bfseries}
\subsectionfont{\normalfont\sffamily\bfseries}
\hypersetup{colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\usepackage[backend=bibtex,
            style=numeric,
            firstinits=true,
            natbib=true]{biblatex}
\bibliography{report}

% Notation and macros
\input{notation.tex}

% Preamble
\title{\sffamily{\emph{revrand}: Technical Report}}
\author{Daniel Steinberg, Louis Tiao \\
    Data61 $|$ CSIRO \\
    email: \texttt{\{firstname.lastname\}@data61.csiro.au}
}
\date{}

\begin{document}

\maketitle
\vspace{-0.5cm}
\noindent\makebox[\linewidth]{\rule{\linewidth}{0.8pt}}
\vspace{0.3cm}

\begin{abstract}
    This is a technical report on the \emph{revrand} software library. This
    library implements various Bayesian linear models (Bayesian linear
    regression), approximate Gaussian processes and generalised linear models.
    These algorithms have been implemented such that they can be used for
    large-scale inference by using stochastic gradients. All of the algorithms
    in \emph{revrand} use a unified feature composition framework that allows
    for easy concatenation and selective application of regression basis
    functions.
\end{abstract}

\tableofcontents

\section{Core Algorithms}

\subsection{Stochastic Gradients and Variational Objective Functions}
\label{sub:stochvar}

Stochastic Gradients is now a ubiquitous method for optimisation when a whole
dataset does not fit in memory, or when optimisation has to be distributed
amongst many computational nodes.

When an objective function factorises over data,
\begin{equation}
    \ffunc{\ins, \param}{} = \sum_{\inss \in \ins} \ffunc{\inss, \param}{},
\end{equation}
a regular gradient descent would perform the following iterations to minimise
the function w.r.t.\ $\param$,
\begin{equation}
    \param_k := \param_{k-1} - \lrate_k \sum_{\inss \in \ins}
    \nabla_{\param} \ffunc{\inss_n, \param}{}\!|_{\param = \param_{k-1}},
\end{equation}
where $\lrate_k$ is the learning rate (step size) at iteration $k$. Stochastic
gradients proposes the following update,
\begin{equation}
    \param_k := \param_{k-1} - \lrate_k \sum_{\inss \in \minibatch}
    \nabla_{\param} \ffunc{\inss, \param}{}\!|_{\param = \param_{k-1}},
    \label{eq:sg}
\end{equation}
where $\minibatch \subset \ins$ is a mini-batch of the original dataset, where
$|\minibatch| \ll |\ins|$.

Unfortunately some objective functions to not entirely decompose over the data,
i.e. 
\begin{equation}
    \ffunc{\ins, \param}{} = \sum_{\inss \in \ins} \ffunc{\inss, \param}{}
    + \func{g}{\param}{}.
    \label{eq:objwconst}
\end{equation}
Let $M = |\minibatch|$ and $N = |\ins|$, then we divide the contribution of the
constant term amongst the mini-batches in stochastic gradients,
\begin{equation}
    \param_k := \param_{k-1} - \lrate_k \sum_{\inss \in \minibatch}
    \nabla_{\param} \ffunc{\inss, \param}{}\!|_{\param = \param_{k-1}}
    - \lrate_k \frac{M}{N} \nabla_{\param} 
    \func{g}{\param}{}\!|_{\param = \param_{k-1}}.
    \label{eq:wsg}
\end{equation} 
This is particularly relevant for variational inference where the evidence
lower bound objective has a component independent of the data. For example, 
lets consider the model,
\begin{align}
    \text{Likelihood:} \quad &\prod^N_{n=1} \probC{\targs_n}{\param}, \\
    \text{prior:} \quad &\probC{\param}{\hyper},
\end{align}
where we want to learn the values of the hyper-parameters, $\hyper$. Minimising
negative log-marginal likelihood is a good objective in this instance, since we
don't care about the value(s) of $\param$,
\begin{equation}
    \argmin_\hyper - \log \int \prod^N_{n=1} \probC{\targs_n}{\param}
    \probC{\param}{\hyper} d \param.
    \label{eq:lml}
\end{equation}
There are two problems with this objective however, (1) it may not factor over
data and (2) the integral may be intractable, for instance, if the prior and
likelihood are not conjugate. In variational inference we use Jensen's
inequality to lower-bound log-marginal likelihood with a tractable objective
function called the evidence lower bound (ELBO),
\begin{align}
    \log \probC{\targ}{\hyper} =& \log \int 
        \prod^N_{n=1} \probC{\targs_n}{\param} 
        \probC{\param}{\hyper} d \param \nonumber \\
        =& \log \int 
        \frac{\prod_n \probC{\targs_n}{\param} \probC{\param}{\hyper}}
        {\qrob{\param}} \qrob{\param} d \param \nonumber \\
        \geq& \int \qrob{\param} \log \sbrac{%
            \frac{\prod_n \probC{\targs_n}{\param} 
            \probC{\param}{\hyper}}{\qrob{\param}}}
        d \param
\end{align}
where $\qrob{\param}$ is an approximation of $\probC{\param}{\hyper}$ that 
makes inference easier. This can be re-written as,
\begin{equation}
    \elbo = \sum^N_{n=1} \expec{q}{\log\probC{\targs_n}{\param}} -
    \KL{\qrob{\param}}{\probC{\param}{\hyper}},
    \label{eq:elbo}
\end{equation}
which takes the form of Equation~\eqref{eq:objwconst}, and so we can weight the
Kullback-Leibler term like the constant term, $\func{g}{\cdot}{}$, from
Equation~\eqref{eq:wsg} if we use stochastic gradients optimisation.
Furthermore, if $\qrob{\param} = \probC{\param}{\hyper}$ then the lower bound
is tight, and this will be equivalent to optimising log-marginal likelihood.


\subsection{Bayesian Linear Regression}

The first machine learning algorithm in revrand is a simple Bayesian linear
regressor of the following form,
\begin{align}
    \text{Likelihood:} \quad &\prod^N_{n=1} 
    \gausC{\targs_n}{\feats_n\T\weights, \var}, \\
    \text{prior:} \quad &\gausC{\weights}{\mathbf{0}, \reg\ident{D}},
\end{align}
where $\feats_n := \func{\featfunc}{\inss_n, \param}{}$ is a feature, or
basis, function that $\featfunc: \real{d} \to \real{D}$. This is, for all
intents and purposes the same algorithm described in~\cite[Chapter
2]{Rasmussen2006}.

We then:
\begin{itemize}
    \item Optimise $\var, \reg$ and $\param$ w.r.t.\ log-marginal likelihood,
        \begin{equation}
            \log \probC{\targ}{\var, \reg, \param} =
            \log \gausC{\targ}{\mathbf{0}, \var\ident{N} + \reg \feat\T\feat},
        \end{equation}
        where $\feat \in \real{N \times D}$ is the concatenation of all the
        features, $\feats_n$. Note this results in the covariance of the
        log-marginal likelihood being $N \times N$, though we can use the
        Woodbury identity to simplify the corresponding matrix inversion.
    \item Solve analytically for the posterior over weights, $\weights | \targ
        \sim \gaus{\pomean, \pocov}$ given the above hyperparameters, where,
        \begin{align*}
            \pocov &= \sbrac{\reg \ident{D} + \frac{1}{\var}
                \feat\T \feat}\inv \\
            \pomean &= \frac{1}{\var} \pocov \feat\T \targ
        \end{align*}
    \item Use the predictive distribution
        \begin{align}
            \probC{\targs\test}{\targ, \ins, \inss\test} &= \int
            \gausC{\targs\test}{\feats\testT\weights, \var}
            \gausC{\weights}{\pomean, \pocov} d\weights \nonumber \\
            &= \gausC{\targs\test}{\feats\testT \pomean,
                \var + \feats\testT \pocov \feats\test}
        \end{align}
        for query inputs, $\inss\test$.
\end{itemize}

It is actually easier to use the ELBO form with stochastic gradients for
learning the parameters of this algorithm, rather than log-marginal likelihood.
This is because it is plainly in the same form as Equation
\eqref{eq:objwconst}, though it would give the same result as log-marginal
likelihood, since no approximation is made about the posterior. The ELBO for
this model is,
\begin{equation}
    \text{TODO}
\end{equation}

We have not implemented a stochastic gradient version of this algorithm since
it still requires a matrix solve involving a $D \times D$ matrix, and so is
\bigo{D^3} in complexity, per iteration. This is true even if we optimise the
posterior covariance directly (or some triangular form). The GLM presented in
the next section circumvents this issue, and is more suited to really large $N$
and $D$ problems.


\subsection{Bayesian Generalised Linear Models}

The algorithm of primary interest in \emph{revrand} is the Bayesian generalised
linear model. The general form of the model implemented by this algorithm is,
\begin{align}
    \text{Likelihood:} \quad &\prod^N_{n=1} 
        \probC{\targs_n}{\activ{\feats_n\T\weights}, \lparam}, 
        \label{eq:glmlike} \\
    \text{prior:} \quad &\gausC{\weights}{\mathbf{0}, \reg\ident{D}},
\end{align}
for an arbitrary univariate likelihood, $\prob{\cdot}$, with an appropriate
transformation (inverse link) function, $\activ{\cdot}$, and parameter(s),
$\lparam$. 

Naturally, both calculating the exact posterior over the weights,
$\probC{\weights}{\targ, \ins}$, and the log-marginal likelihood,
$\prob{\targ}$, for hyperparameter learning are intractable since we may have a
non-conjugate relationship between the likelihood and prior. Therefore we must
resort to approximating the true posterior and the log-marginal likelihood.
We use a modification of the nonparametric variational algorithm
presented in~\cite{gershman2012} for approximating the posterior and log
marginal likelihood. The posterior takes the form
\begin{align}
    \probC{\weights}{\targ, \ins} &\approx \qrob{\weights}, \nonumber \\
    &= \frac{1}{K} \prod^K_{k=1} \gausC{\weights}{\pomean_k, \dpocov_k},
\end{align}
i.e.\ a mixture of $K$ diagonal Gaussians, $\dpocov_k = \diag{[\dpocovs_{k,1},
    \ldots, \dpocovs_{k, D}]\T}$. This is a very flexible form for the
approximate posterior, and still admits a closed-form lower bound to the
log-marginal likelihood. Furthermore this has the nice property that our
algorithm no longer has a \bigo{D^3} cost associated with the number of
features. \citet{gershman2012} make one more approximation based on local
second order Taylor expansions of the joint,
\begin{multline}
    \expec{q}{\log \prob{\targ, \weights}} \approx
    \frac{1}{K} \sum^K_{k=1} \sum^N_{n=1} 
    \log \probC{\targs_n}{\activ{\feats_n\T \pomean_k}, \lparam} \\
    + \log \probC{\weights}{\pomean_k, \dpocov_k}
    + \frac{1}{2} \trace{\dpocov_k\hess{k}},
\end{multline}
to deal with non-conjugacy within their variational framework. Here $\hess{k} =
\nabla^2_{\weights} \prob{\targs, \weights}\!|_{\weights =
    \pomean_k}$\footnote{We only need the diagonals of this Hessian matrix.}.
Unfortunately this approximation to the ELBO actually breaks the lower bound,
but works well in practice. For more details on this inference scheme we direct
the reader to~\cite{gershman2012}. The final objective function we optimise is,
\begin{multline}
    \elbo \approx
    \frac{1}{K} \sum^K_{k=1} \sum^N_{n=1} 
    \log \probC{\targs_n}{\activ{\feats_n\T \pomean_k}, \lparam}
    + \log \probC{\weights}{\pomean_k, \dpocov_k} \\
    + \frac{1}{2} \trace{\dpocov_k\hess{k}}
    - \log \brac{\sum^K_{j=1} \gausC{\pomean_k}{\pomean_j, \dpocov_k +
            \dpocov_j}} - \log K. \label{eq:glmobj}
\end{multline}
While this is an approximation to Equation~\eqref{eq:elbo}, it retains the
property that it easily factorises over the data ($n$), and so can be optimised
using stochastic gradients as described in \S \ref{sub:stochvar}.

In \emph{revrand} we actually optimise Equation~\eqref{eq:glmobj} with respect
to the hyperparameters, $\param, \lparam$ \emph{and} the parameters $\pomean_k,
\dpocov_k~\forall k$ simultaneously. This is unlike the optimisation scheme 
in~\cite{gershman2012}, since it requires \nth{3} derivatives of the
likelihood in Equation~\eqref{eq:glmlike} w.r.t.\ $\weights$. However, we find
that providing these derivatives is simple (for our model) compared to the more
complex optimisation scheme presented in~\cite{gershman2012}, which is not as
amenable to large datasets as stochastic gradients when hyperparameter 
optimisation is involved.

TODO: Prediction

\subsection{Large Scale Gaussian Process Approximation}

TODO: Show kernel approximations (from notebook)


\subsubsection{With Bayesian Linear Regression}

i.e.\ A la Carte~\cite{yang2014}

we still need to invert a $D \times D$ matrix with stochastic gradients, so
\bigo{D^3}, but that is where the GLM comes in.


\subsubsection{With Bayesian GLMs}

We only have to invert $K$ diagonal matrices of dimension $D \times D$, so
\bigo{KD}.

\section{Feature Composition Framework}

\section{Experiments}


\printbibliography%

\end{document}
